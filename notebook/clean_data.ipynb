{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse / Extract consistent needed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Kim Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path(\"..\") / \"data\"\n",
    "INPUT_PATH = DATA_PATH / \"raw\" / \"2023_Kim.xlsx\"\n",
    "OUTPUT_PATH = DATA_PATH / \"interim\" / \"toxprot_2017_old.csv\"\n",
    "\n",
    "# Define constants\n",
    "SHEET_NAME = \"ToxProt11.2017\"\n",
    "COLUMNS_TO_EXTRACT = [\n",
    "    \"Entry\",\n",
    "    \"Organism\",\n",
    "    \"Protein families\",\n",
    "    \"Length (aa)\",\n",
    "    \"Fragments\",\n",
    "    \"Toxic dose\",\n",
    "    \"PTM\",\n",
    "]\n",
    "\n",
    "# Read specific columns from the Excel sheet\n",
    "df = pd.read_excel(INPUT_PATH, sheet_name=SHEET_NAME, usecols=COLUMNS_TO_EXTRACT)\n",
    "\n",
    "# Rename 'Length (aa)' to 'Length' and 'Fragments' to 'Fragment'\n",
    "df = df.rename(columns={\"Length (aa)\": \"Length\", \"Fragments\": \"Fragment\"})\n",
    "\n",
    "# Ensure output directory exists and save as CSV\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# Display information about the processed data\n",
    "print(f\"Data extracted and saved to {OUTPUT_PATH}\")\n",
    "print(f\"Shape of the extracted data: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"First few rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse SwissProt 2017-11 (extracted from [FTP](https://ftp.uniprot.org/pub/databases/uniprot/previous_major_releases/release-2017_11/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_protfams(df):\n",
    "    # Split on common delimiters and take first part\n",
    "    df[\"Protein families\"] = df[\"Protein families\"].str.split(r\"[.,;]\").str[0]\n",
    "    print(\n",
    "        f\"Unique protein families after splitting: {df['Protein families'].nunique()}\"\n",
    "    )\n",
    "\n",
    "    # Map of family name corrections\n",
    "    family_corrections = {\n",
    "        \"I1 superfamily\": \"Conotoxin I1 superfamily\",\n",
    "        \"O1 superfamily\": \"Conotoxin O1 superfamily\",\n",
    "        \"O2 superfamily\": \"Conotoxin O2 superfamily\",\n",
    "        \"E superfamily\": \"Conotoxin E superfamily\",\n",
    "        \"F superfamily\": \"Conotoxin F superfamily\",\n",
    "        \"Conotoxin M family\": \"Conotoxin M superfamily\",\n",
    "        \"Conotoxin B2 family\": \"Conotoxin B2 superfamily\",\n",
    "        \"Conotoxin O1 family\": \"Conotoxin O1 superfamily\",\n",
    "        \"Conotoxin O2 family\": \"Conotoxin O2 superfamily\",\n",
    "    }\n",
    "\n",
    "    # Apply all corrections at once\n",
    "    df[\"Protein families\"] = df[\"Protein families\"].replace(family_corrections)\n",
    "    print(\n",
    "        f\"Unique protein families after processing: {df['Protein families'].nunique()}\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the generalizable function to create a FASTA file\n",
    "def create_fasta_file(\n",
    "    df: pd.DataFrame,\n",
    "    entry_col: str,\n",
    "    sequence_col: str,\n",
    "    fasta_output_path: Path,\n",
    "    signal_peptide_range_column: str = None,\n",
    "):\n",
    "    \"\"\"Creates a FASTA file from a DataFrame, optionally removing signal peptides.\"\"\"\n",
    "    fasta_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(fasta_output_path, \"w\") as f_out:\n",
    "        for _, row in df.iterrows():\n",
    "            entry = row[entry_col]\n",
    "            original_sequence = row[sequence_col]\n",
    "\n",
    "            # Ensure sequence is a valid non-empty string\n",
    "            if not isinstance(original_sequence, str) or not original_sequence:\n",
    "                continue\n",
    "\n",
    "            sequence_to_write = original_sequence\n",
    "\n",
    "            # If a signal peptide range column is provided, attempt to remove the signal peptide\n",
    "            if signal_peptide_range_column:\n",
    "                signal_range_str = row.get(signal_peptide_range_column)\n",
    "\n",
    "                # Process if range string is a valid string and contains a hyphen (e.g., \"1-22\")\n",
    "                if isinstance(signal_range_str, str) and \"-\" in signal_range_str:\n",
    "                    try:\n",
    "                        # Signal peptide range (e.g., \"1-22\") is 1-based; slice at end_pos_1based for mature protein.\n",
    "                        end_pos_1based = int(signal_range_str.split(\"-\")[1])\n",
    "\n",
    "                        if end_pos_1based > 0:\n",
    "                            # Slice from end_pos_1based. Python handles end_pos_1based >= len correctly (empty string).\n",
    "                            sequence_to_write = original_sequence[end_pos_1based:]\n",
    "                        # If end_pos_1based is not positive, original_sequence is kept (invalid range for cut).\n",
    "                    except (ValueError, IndexError):\n",
    "                        # If parsing fails (e.g. \"1-foo\", \"1-\", \"-\"), keep the original sequence.\n",
    "                        pass\n",
    "\n",
    "            # Write to FASTA only if the (potentially modified) sequence is not empty\n",
    "            if sequence_to_write:\n",
    "                f_out.write(f\">{entry}\\n\")\n",
    "                f_out.write(f\"{sequence_to_write}\\n\")\n",
    "\n",
    "    print(f\"FASTA file created at {fasta_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_toxprot_tsv(\n",
    "    tsv_input_path: Path,\n",
    "    update_protfams_func,\n",
    "    create_fasta_func,\n",
    "    display_func=display,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a ToxProt TSV file:\n",
    "    - Reads the TSV\n",
    "    - Renames columns for consistency\n",
    "    - Updates protein families\n",
    "    - Merges Gene Ontology columns\n",
    "    - Writes a cleaned CSV (without sequence and original GO columns)\n",
    "    - Writes a FASTA file (removing signal peptide if present)\n",
    "    - Displays summary info\n",
    "\n",
    "    Args:\n",
    "        tsv_input_path (Path): Path to the input TSV file.\n",
    "        update_protfams_func (callable): Function to update protein families.\n",
    "        create_fasta_func (callable): Function to create FASTA file.\n",
    "        display_func (callable, optional): Function to display DataFrame (default: display).\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Infer output paths\n",
    "    base = tsv_input_path.with_suffix(\"\")\n",
    "    csv_output_path = base.with_suffix(\".csv\")\n",
    "    fasta_output_path = base.with_suffix(\".fasta\")\n",
    "\n",
    "    # Read columns from the TSV file\n",
    "    # Try to infer columns present in the file\n",
    "    with open(tsv_input_path, \"r\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "    # Required columns\n",
    "    required_cols = [\n",
    "        \"Entry\",\n",
    "        \"Organism\",\n",
    "        \"Organism (ID)\",\n",
    "        \"Protein families\",\n",
    "        \"Length\",\n",
    "        \"Fragment\",\n",
    "        \"Toxic dose\",\n",
    "        \"Post-translational modification\",\n",
    "        \"Sequence\",\n",
    "        \"Signal peptide (range)\",\n",
    "        \"Protein existence\",\n",
    "    ]\n",
    "    # Optional GO columns\n",
    "    go_cols = [\n",
    "        \"Gene Ontology (GO)\",\n",
    "        \"Gene Ontology (biological process)\",\n",
    "        \"Gene Ontology (cellular component)\",\n",
    "        \"Gene Ontology (molecular function)\",\n",
    "    ]\n",
    "    # Only use columns that exist in the file\n",
    "    usecols = [col for col in required_cols + go_cols if col in header]\n",
    "\n",
    "    df = pd.read_csv(tsv_input_path, sep=\"\\t\", usecols=usecols)\n",
    "\n",
    "    # Rename 'Post-translational modification' to 'PTM' if present\n",
    "    if \"Post-translational modification\" in df.columns:\n",
    "        df = df.rename(columns={\"Post-translational modification\": \"PTM\"})\n",
    "\n",
    "    # Update protein families\n",
    "    df = update_protfams_func(df)\n",
    "\n",
    "    # Merge GO columns if present\n",
    "    go_merge_cols = [\n",
    "        col\n",
    "        for col in [\n",
    "            \"Gene Ontology (biological process)\",\n",
    "            \"Gene Ontology (cellular component)\",\n",
    "            \"Gene Ontology (molecular function)\",\n",
    "        ]\n",
    "        if col in df.columns\n",
    "    ]\n",
    "    if go_merge_cols:\n",
    "\n",
    "        def merge_go_terms(row):\n",
    "            terms = []\n",
    "            for col in go_merge_cols:\n",
    "                val = row.get(col)\n",
    "                if pd.notnull(val):\n",
    "                    val = str(val).strip()\n",
    "                    if val:\n",
    "                        terms.append(val)\n",
    "            return \"; \".join(terms) if terms else pd.NA\n",
    "\n",
    "        df[\"Gene Ontology (GO)\"] = df.apply(merge_go_terms, axis=1)\n",
    "\n",
    "    # Create FASTA file if 'Sequence' and 'Entry' columns exist\n",
    "    if \"Sequence\" in df.columns and \"Entry\" in df.columns:\n",
    "        signal_peptide_col = (\n",
    "            \"Signal peptide (range)\" if \"Signal peptide (range)\" in df.columns else None\n",
    "        )\n",
    "        create_fasta_func(\n",
    "            df, \"Entry\", \"Sequence\", fasta_output_path, signal_peptide_col\n",
    "        )\n",
    "\n",
    "    # Prepare columns for CSV output: remove 'Sequence', 'Signal peptide (range)', and original GO columns\n",
    "    drop_cols = [\"Sequence\", \"Signal peptide (range)\"] + go_merge_cols\n",
    "    columns_for_csv = [col for col in df.columns if col not in drop_cols]\n",
    "\n",
    "    # Save CSV\n",
    "    df.to_csv(csv_output_path, index=False, columns=columns_for_csv)\n",
    "\n",
    "    # Display info\n",
    "    print(f\"TSV data processed. FASTA file created at {fasta_output_path}.\")\n",
    "    print(\n",
    "        f\"CSV data (without sequence and original GO columns) saved to {csv_output_path}\"\n",
    "    )\n",
    "\n",
    "    df_csv_content_view = df[columns_for_csv]\n",
    "    print(f\"Shape of the data saved to CSV: {df_csv_content_view.shape}\")\n",
    "    print(f\"Columns in CSV: {df_csv_content_view.columns.tolist()}\")\n",
    "    print(\"First few rows (as saved to CSV):\")\n",
    "    display_func(df_csv_content_view.head())\n",
    "\n",
    "\n",
    "# Process 2017 dataset\n",
    "process_toxprot_tsv(\n",
    "    DATA_PATH / \"interim\" / \"toxprot_2017.tsv\",\n",
    "    update_protfams,\n",
    "    create_fasta_file,\n",
    "    display_func=display,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse SwissProt release: [2025-01](https://ftp.uniprot.org/pub/databases/uniprot/previous_major_releases/release-2025_01/knowledgebase/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 2025 dataset\n",
    "process_toxprot_tsv(\n",
    "    DATA_PATH / \"interim\" / \"toxprot_2025.tsv\",\n",
    "    update_protfams,\n",
    "    create_fasta_file,\n",
    "    display_func=display,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get taxonomy info\n",
    "\n",
    "Process taxonomic information from a CSV file using taxopy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import taxopy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def setup_db_paths():\n",
    "    \"\"\"Setup and return the database paths.\"\"\"\n",
    "    home_dir = Path.home() / \".cache\"\n",
    "    db_dir = home_dir / \"taxopy_db\"\n",
    "    db_dir.mkdir(parents=True, exist_ok=True)\n",
    "    nodes_file = db_dir / \"nodes.dmp\"\n",
    "    names_file = db_dir / \"names.dmp\"\n",
    "    merged_file = db_dir / \"merged.dmp\"\n",
    "\n",
    "    return db_dir, nodes_file, names_file, merged_file\n",
    "\n",
    "\n",
    "def initialize_taxdb():\n",
    "    \"\"\"Initialize and return the taxonomy database.\"\"\"\n",
    "    # Get the database paths\n",
    "    db_dir, nodes_file, names_file, merged_file = setup_db_paths()\n",
    "\n",
    "    if nodes_file.exists() and names_file.exists():\n",
    "        print(f\"Loading existing taxopy database from {db_dir}\")\n",
    "        taxdb = taxopy.TaxDb(\n",
    "            nodes_dmp=str(nodes_file),\n",
    "            names_dmp=str(names_file),\n",
    "            merged_dmp=str(merged_file),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Downloading taxopy database to {db_dir}\")\n",
    "        taxdb = taxopy.TaxDb(taxdb_dir=str(db_dir), keep_files=True)\n",
    "\n",
    "    return taxdb\n",
    "\n",
    "\n",
    "def get_taxonomy_info(taxon_id, taxdb):\n",
    "    \"\"\"Get order, family, genus, species info for a taxon ID.\"\"\"\n",
    "    # Get the Taxon object\n",
    "    taxon = taxopy.Taxon(taxon_id, taxdb)\n",
    "\n",
    "    # Get the rank information\n",
    "    ranks = taxon.rank_name_dictionary\n",
    "\n",
    "    return {\n",
    "        \"taxon_name\": taxon.name,\n",
    "        \"phylum\": ranks.get(\"phylum\", \"\"),\n",
    "        \"class\": ranks.get(\"class\", \"\"),\n",
    "        \"order\": ranks.get(\"order\", \"\"),\n",
    "        \"family\": ranks.get(\"family\", \"\"),\n",
    "        \"genus\": ranks.get(\"genus\", \"\"),\n",
    "        \"species\": ranks.get(\"species\", \"\"),\n",
    "    }  # superkingdom, kingdom, phylum, class, order, family, genus, species\n",
    "\n",
    "\n",
    "def build_taxonomy_cache(df, taxdb):\n",
    "    \"\"\"Build a cache of taxonomy information for all unique organism IDs.\"\"\"\n",
    "    taxonomy_cache = {}\n",
    "    for taxon_id in df[\"Organism (ID)\"].unique():\n",
    "        if pd.notna(taxon_id):\n",
    "            taxonomy_cache[taxon_id] = get_taxonomy_info(taxon_id, taxdb)\n",
    "\n",
    "    return taxonomy_cache\n",
    "\n",
    "\n",
    "def add_taxonomy_columns(df, taxonomy_cache):\n",
    "    \"\"\"Add taxonomy columns to the dataframe.\"\"\"\n",
    "\n",
    "    # Create a mapping function that extracts all taxonomy info at once\n",
    "    def get_taxonomy_info(taxon_id):\n",
    "        cache_entry = taxonomy_cache.get(taxon_id, {})\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"Scientific_Name\": cache_entry.get(\"taxon_name\", \"\"),\n",
    "                \"Phylum\": cache_entry.get(\"phylum\", \"\"),\n",
    "                \"Class\": cache_entry.get(\"class\", \"\"),\n",
    "                \"Order\": cache_entry.get(\"order\", \"\"),\n",
    "                \"Family\": cache_entry.get(\"family\", \"\"),\n",
    "                \"Genus\": cache_entry.get(\"genus\", \"\"),\n",
    "                \"Species\": cache_entry.get(\"species\", \"\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Apply the mapping function once to get all columns\n",
    "    taxonomy_df = df[\"Organism (ID)\"].apply(get_taxonomy_info)\n",
    "\n",
    "    # Concatenate the new columns with the original dataframe\n",
    "    return pd.concat([df, taxonomy_df], axis=1)\n",
    "\n",
    "\n",
    "def process_dataframe(input_path, output_path, taxdb):\n",
    "    \"\"\"Process the dataframe: load, add taxonomy, remove Organism column, save.\"\"\"\n",
    "    # Load the dataframe\n",
    "    print(f\"Loading data from {input_path}\")\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Build the taxonomy cache\n",
    "    print(\"Building taxonomy cache...\")\n",
    "    taxonomy_cache = build_taxonomy_cache(df, taxdb)\n",
    "\n",
    "    # Add taxonomy columns\n",
    "    print(\"Adding taxonomy columns...\")\n",
    "    df = add_taxonomy_columns(df, taxonomy_cache)\n",
    "\n",
    "    # Remove the Organism column if it exists\n",
    "    if \"Organism\" in df.columns:\n",
    "        print(\"Removing 'Organism' column...\")\n",
    "        df = df.drop(columns=[\"Organism\"])\n",
    "\n",
    "    # Save the updated dataframe\n",
    "    print(f\"Saving processed data to {output_path}\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Processing complete. Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing taxonomy database...\")\n",
    "taxdb = initialize_taxdb()\n",
    "\n",
    "# Process ToxProt 2017-11\n",
    "input_path = \"../data/interim/toxprot_2017.csv\"\n",
    "output_path = \"../data/processed/toxprot_2017.csv\"\n",
    "process_dataframe(input_path, output_path, taxdb)\n",
    "\n",
    "# Process ToxProt 2025-03\n",
    "input_path = \"../data/interim/toxprot_2025.csv\"\n",
    "output_path = \"../data/processed/toxprot_2025.csv\"\n",
    "process_dataframe(input_path, output_path, taxdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiate between marine and terrestrial organism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the marine/terrestrial mapping\n",
    "mapping_path = \"../data/raw/marine_terrestrial.json\"\n",
    "\n",
    "# Load the marine/terrestrial mapping\n",
    "with open(mapping_path, \"r\") as f:\n",
    "    habitat_mapping = json.load(f)\n",
    "\n",
    "\n",
    "# Function to determine habitat based on order and genus\n",
    "def determine_habitat(row):\n",
    "    order = row[\"Order\"]\n",
    "    genus = row.get(\"Genus\", \"\")  # Get genus if available, otherwise empty string\n",
    "\n",
    "    # Check if order is in clear_orders\n",
    "    if order in habitat_mapping[\"clear_orders\"][\"terrestrial\"]:\n",
    "        return \"terrestrial\"\n",
    "    elif order in habitat_mapping[\"clear_orders\"][\"marine\"]:\n",
    "        return \"marine\"\n",
    "\n",
    "    # Check if order is in ambiguous_orders\n",
    "    if order in habitat_mapping[\"ambiguous_orders\"]:\n",
    "        # Check if genus is in the terrestrial list for this order\n",
    "        if genus in habitat_mapping[\"ambiguous_orders\"][order].get(\"terrestrial\", {}):\n",
    "            return \"terrestrial\"\n",
    "        # Check if genus is in the marine list for this order\n",
    "        elif genus in habitat_mapping[\"ambiguous_orders\"][order].get(\"marine\", {}):\n",
    "            return \"marine\"\n",
    "\n",
    "    # If we can't determine, return 'unknown'\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "# Process 2017 dataset\n",
    "csv_path_2017 = \"../data/processed/toxprot_2017.csv\"\n",
    "df_2017 = pd.read_csv(csv_path_2017)\n",
    "df_2017[\"Habitat\"] = df_2017.apply(determine_habitat, axis=1)\n",
    "print(\"ToxProt 2017 habitat distribution:\")\n",
    "print(df_2017[\"Habitat\"].value_counts())\n",
    "df_2017.to_csv(csv_path_2017, index=False)\n",
    "print(f\"Updated {csv_path_2017} with habitat information\")\n",
    "\n",
    "# Process 2025 dataset\n",
    "csv_path_2025 = \"../data/processed/toxprot_2025.csv\"\n",
    "df_2025 = pd.read_csv(csv_path_2025)\n",
    "df_2025[\"Habitat\"] = df_2025.apply(determine_habitat, axis=1)\n",
    "print(\"\\nToxProt 2025 habitat distribution:\")\n",
    "print(df_2025[\"Habitat\"].value_counts())\n",
    "df_2025.to_csv(csv_path_2025, index=False)\n",
    "print(f\"Updated {csv_path_2025} with habitat information\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
